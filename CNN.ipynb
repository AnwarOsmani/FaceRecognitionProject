{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnwarOsmani/FaceRecognitionProject/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data Preparation with more augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding=4),  # Random crop\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model Definition with additional Conv Layers, Batch Normalization, and more Dropout\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # Additional convolution\n",
        "        self.bn4 = nn.BatchNorm2d(256)  # Batch Normalization for the new layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)  # Adjusted for new convolution layer\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(self.relu(self.bn4(self.conv4(x))))  # New convolution layer\n",
        "        x = x.view(-1, 256 * 2 * 2)  # Adjusted for the new conv layer\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Model, Loss Function, and Optimizer\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop with more epochs\n",
        "epochs = 40  # Increased number of epochs\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "# Evaluate on Test Data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save the Trained Model\n",
        "torch.save(model.state_dict(), 'cnn_cifar10.pth')\n",
        "print(\"Model saved as cnn_cifar10.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apLEga5ULhGB",
        "outputId": "a19f3b0f-70c2-4082-d372-9cca31f1c1ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/40, Loss: 1.5992137077824233\n",
            "Epoch 2/40, Loss: 1.2483443392969458\n",
            "Epoch 3/40, Loss: 1.0926013864062327\n",
            "Epoch 4/40, Loss: 0.9890044439021889\n",
            "Epoch 5/40, Loss: 0.9184940310237962\n",
            "Epoch 6/40, Loss: 0.8622032237784637\n",
            "Epoch 7/40, Loss: 0.824799417077428\n",
            "Epoch 8/40, Loss: 0.7807503020214608\n",
            "Epoch 9/40, Loss: 0.7589170508982276\n",
            "Epoch 10/40, Loss: 0.7234487763374967\n",
            "Epoch 11/40, Loss: 0.6964443902987654\n",
            "Epoch 12/40, Loss: 0.6782793647531048\n",
            "Epoch 13/40, Loss: 0.6565317178473753\n",
            "Epoch 14/40, Loss: 0.6413453148149163\n",
            "Epoch 15/40, Loss: 0.6277482290859417\n",
            "Epoch 16/40, Loss: 0.6085263859966527\n",
            "Epoch 17/40, Loss: 0.598341470026909\n",
            "Epoch 18/40, Loss: 0.583225298377559\n",
            "Epoch 19/40, Loss: 0.5731234988745522\n",
            "Epoch 20/40, Loss: 0.564718511384314\n",
            "Epoch 21/40, Loss: 0.5494528618424445\n",
            "Epoch 22/40, Loss: 0.539250733869155\n",
            "Epoch 23/40, Loss: 0.5354446681487895\n",
            "Epoch 24/40, Loss: 0.5240206128305487\n",
            "Epoch 25/40, Loss: 0.5142026329627427\n",
            "Epoch 26/40, Loss: 0.50814737533898\n",
            "Epoch 27/40, Loss: 0.5021307464984371\n",
            "Epoch 28/40, Loss: 0.49667171013477207\n",
            "Epoch 29/40, Loss: 0.48419270186168156\n",
            "Epoch 30/40, Loss: 0.48010386551356377\n",
            "Epoch 31/40, Loss: 0.4733382509568768\n",
            "Epoch 32/40, Loss: 0.46593858513152203\n",
            "Epoch 33/40, Loss: 0.46412371683989645\n",
            "Epoch 34/40, Loss: 0.4581176107153868\n",
            "Epoch 35/40, Loss: 0.4571132123889521\n",
            "Epoch 36/40, Loss: 0.4449727068014462\n",
            "Epoch 37/40, Loss: 0.4439148695953667\n",
            "Epoch 38/40, Loss: 0.43892667111952594\n",
            "Epoch 39/40, Loss: 0.434746394033932\n",
            "Epoch 40/40, Loss: 0.42610159146663784\n",
            "Test Accuracy: 81.57%\n",
            "Model saved as cnn_cifar10.pth\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9jQEBBYP1GsxCNfRGCAT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}